{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6748a7ca-6efd-4a2f-a70b-e2e9c94276fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (0.3.25)\n",
      "Requirement already satisfied: langchain-openai in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (0.3.18)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from langchain) (0.3.63)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from langchain) (0.3.43)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from langchain) (2.11.5)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.68.2 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from langchain-openai) (1.82.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (4.13.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.68.2->langchain-openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbc4f492-da4a-47c2-ab27-1065b5ad9b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESTABLECIENDO CLAVE DE LangSmith\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "INGRESA LA API KEY DE LANGSMITH:  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESTABLECIENDO CLAVES PUBLICAS\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "import getpass\n",
    "print(\"ESTABLECIENDO CLAVE DE LangSmith\")\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"INGRESA LA API KEY DE LANGSMITH: \")\n",
    "print(\"ESTABLECIENDO CLAVES PUBLICAS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "815dfc06-4431-47f5-bfcb-391ebb67a6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -qU \"langchain[google-genai]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3922e532-231f-4ca2-a389-765ef3d73cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECCIONANDO MODELO DE CHAT\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "INGRESA LA API KEY DE GOOGLE GEMINI:  ········\n"
     ]
    }
   ],
   "source": [
    "print(\"SELECCIONANDO MODELO DE CHAT\")\n",
    "import getpass\n",
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"INGRESA LA API KEY DE GOOGLE GEMINI: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0f6967d-11be-4064-b388-cdf40ceebd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-ollama\n",
      "  Downloading langchain_ollama-0.3.3-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting ollama<1.0.0,>=0.4.8 (from langchain-ollama)\n",
      "  Downloading ollama-0.5.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.60 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from langchain-ollama) (0.3.63)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.126 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.60->langchain-ollama) (0.3.43)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.60->langchain-ollama) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.60->langchain-ollama) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.60->langchain-ollama) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.60->langchain-ollama) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.60->langchain-ollama) (4.13.2)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.60->langchain-ollama) (2.11.5)\n",
      "Requirement already satisfied: httpx>=0.27 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from ollama<1.0.0,>=0.4.8->langchain-ollama) (0.28.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from httpx>=0.27->ollama<1.0.0,>=0.4.8->langchain-ollama) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from httpx>=0.27->ollama<1.0.0,>=0.4.8->langchain-ollama) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from httpx>=0.27->ollama<1.0.0,>=0.4.8->langchain-ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from httpx>=0.27->ollama<1.0.0,>=0.4.8->langchain-ollama) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.27->ollama<1.0.0,>=0.4.8->langchain-ollama) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.60->langchain-ollama) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.60->langchain-ollama) (3.10.18)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.60->langchain-ollama) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.60->langchain-ollama) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.60->langchain-ollama) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.60->langchain-ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.60->langchain-ollama) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.60->langchain-ollama) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.60->langchain-ollama) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.60->langchain-ollama) (2.4.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\usuario\\documents\\github\\proyectotallerintegrador\\entidexenterprise\\venv\\lib\\site-packages (from anyio->httpx>=0.27->ollama<1.0.0,>=0.4.8->langchain-ollama) (1.3.1)\n",
      "Downloading langchain_ollama-0.3.3-py3-none-any.whl (21 kB)\n",
      "Downloading ollama-0.5.1-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: ollama, langchain-ollama\n",
      "Successfully installed langchain-ollama-0.3.3 ollama-0.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ac682b8-427e-4c4b-b2e9-176f7296eebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -U -q langchain-huggingface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e47c521-c7e9-40d5-9d4f-58ee228f4ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECCIONANDO MODELO DE EMBEDDINGS\n",
      "Embedding seleccionado\n"
     ]
    }
   ],
   "source": [
    "print(\"SELECCIONANDO MODELO DE EMBEDDINGS\")\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Embedding liviano y especializado\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "print(\"Embedding seleccionado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b40667f-b216-491d-b231-8cf043091cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -U -q chromadb langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c89ccac2-98ce-4495-87fa-e46d50534fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -U -q langchain-core langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79359716-66b2-4917-a1b4-085db06adc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VECTOR STORE USANDO CHROMA\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collectionV2\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    ")\n",
    "print(\"VECTOR STORE USANDO CHROMA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30e499d9-94db-4e16-8761-989599ab189b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Cargando contenido web...\n",
      "✅ CONTENIDO WEB CARGADO. Documentos obtenidos: 1\n",
      "✂️ Dividiendo documentos...\n",
      "📄 Fragmentos generados: 40\n",
      "🧠 Agregando fragmentos al vector store...\n",
      "Fragmento 1/40 agregado\n",
      "Fragmento 2/40 agregado\n",
      "Fragmento 3/40 agregado\n",
      "Fragmento 4/40 agregado\n",
      "Fragmento 5/40 agregado\n",
      "Fragmento 6/40 agregado\n",
      "Fragmento 7/40 agregado\n",
      "Fragmento 8/40 agregado\n",
      "Fragmento 9/40 agregado\n",
      "Fragmento 10/40 agregado\n",
      "Fragmento 11/40 agregado\n",
      "Fragmento 12/40 agregado\n",
      "Fragmento 13/40 agregado\n",
      "Fragmento 14/40 agregado\n",
      "Fragmento 15/40 agregado\n",
      "Fragmento 16/40 agregado\n",
      "Fragmento 17/40 agregado\n",
      "Fragmento 18/40 agregado\n",
      "Fragmento 19/40 agregado\n",
      "Fragmento 20/40 agregado\n",
      "Fragmento 21/40 agregado\n",
      "Fragmento 22/40 agregado\n",
      "Fragmento 23/40 agregado\n",
      "Fragmento 24/40 agregado\n",
      "Fragmento 25/40 agregado\n",
      "Fragmento 26/40 agregado\n",
      "Fragmento 27/40 agregado\n",
      "Fragmento 28/40 agregado\n",
      "Fragmento 29/40 agregado\n",
      "Fragmento 30/40 agregado\n",
      "Fragmento 31/40 agregado\n",
      "Fragmento 32/40 agregado\n",
      "Fragmento 33/40 agregado\n",
      "Fragmento 34/40 agregado\n",
      "Fragmento 35/40 agregado\n",
      "Fragmento 36/40 agregado\n",
      "Fragmento 37/40 agregado\n",
      "Fragmento 38/40 agregado\n",
      "Fragmento 39/40 agregado\n",
      "Fragmento 40/40 agregado\n",
      "✅ Fragmentos agregados al vector store.\n",
      "📥 Cargando prompt predefinido desde LangChain Hub...\n",
      "✅ Prompt cargado correctamente.\n",
      "🔧 Construyendo grafo de estados...\n",
      "✅ Proceso correctamente creado.\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANCIONES Y DEPENDENCIAS\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "# Carga y fragmentación del contenido web\n",
    "print(\"🔄 Cargando contenido web...\")\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "print(f\"✅ CONTENIDO WEB CARGADO. Documentos obtenidos: {len(docs)}\")\n",
    "\n",
    "# División del texto en partes pequeñas\n",
    "print(\"✂️ Dividiendo documentos...\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=100)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "print(f\"📄 Fragmentos generados: {len(all_splits)}\")\n",
    "\n",
    "# Agrega los fragmentos divididos al almacenamiento vectorial (vector_store)\n",
    "print(\"🧠 Agregando fragmentos al vector store...\")\n",
    "for i, doc in enumerate(all_splits, 1):\n",
    "    _ = vector_store.add_documents(documents=[doc])\n",
    "    print(f\"Fragmento {i}/{len(all_splits)} agregado\")\n",
    "print(\"✅ Fragmentos agregados al vector store.\")\n",
    "\n",
    "# Define prompt for question-answering\n",
    "print(\"📥 Cargando prompt predefinido desde LangChain Hub...\")\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "print(\"✅ Prompt cargado correctamente.\")\n",
    "\n",
    "# Definición del estado de la aplicación con tipos\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "# Definición de la función para recuperar documentos\n",
    "def retrieve(state: State):\n",
    "    print(f\"🔍 Buscando contexto para la pregunta: {state['question']}\")\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    print(f\"✅ Documentos recuperados: {len(retrieved_docs)}\")\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "# Definición de la función para generar la respuesta\n",
    "def generate(state: State):\n",
    "    print(\"🧠 Generando respuesta...\")\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    print(\"✅ Respuesta generada.\")\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "# Construcción y compilación del grafo de estados\n",
    "print(\"🔧 Construyendo grafo de estados...\")\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()\n",
    "print(\"✅ Proceso correctamente creado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f083d1b6-4483-473a-890c-612868ffa3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Buscando contexto para la pregunta: ¿Qué es la descomposición de tareas?\n",
      "✅ Documentos recuperados: 4\n",
      "🧠 Generando respuesta...\n",
      "✅ Respuesta generada.\n",
      "La descomposición de tareas es una técnica para mejorar el rendimiento del modelo en tareas complejas. Implica dividir tareas grandes en pasos más pequeños y simples. Esto se puede hacer mediante indicaciones simples, instrucciones específicas de la tarea o aportaciones humanas.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"¿Qué es la descomposición de tareas?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d2c9834-fbfd-4b5a-8157-b54e44b2eac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Buscando contexto para la pregunta: What is Task Decomposition?\n",
      "✅ Documentos recuperados: 4\n",
      "🧠 Generando respuesta...\n",
      "✅ Respuesta generada.\n",
      "I'm sorry, but the provided context does not contain information about task decomposition. Therefore, I cannot answer your question.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b9e171e-7d8b-4d20-a75e-026500f3fb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Buscando contexto para la pregunta: ¿De que trata el articulo?\n",
      "✅ Documentos recuperados: 4\n",
      "🧠 Generando respuesta...\n",
      "✅ Respuesta generada.\n",
      "El artículo trata sobre la arquitectura de agentes generativos y cómo esta simulación resulta en un comportamiento social emergente. Este comportamiento incluye la difusión de información, la memoria de relaciones y la coordinación de eventos sociales. También menciona AutoGPT y sus problemas de confiabilidad.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"¿De que trata el articulo?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6e9903c-d450-449c-b3d2-5581054a88e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 43047\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Only keep post title, headers, and content from the full HTML.\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "assert len(docs) == 1\n",
    "print(f\"Total characters: {len(docs[0].page_content)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93e6b9a2-86ea-4a25-bdc5-3e41c7b9c9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1535bce8-d548-4194-b854-69733db1a249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split blog post into 63 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c10a961-b0d3-4af9-979c-9ce3992e7ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ff56c577-4783-49d3-acd5-08034a7cce93', 'b72a52ff-69fa-4596-874e-3b39f280427d', '23f72733-70dc-4146-81e1-c8e8c077addf']\n"
     ]
    }
   ],
   "source": [
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "173e43f0-425e-4444-87b3-5b79b668c84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Documents\\GitHub\\ProyectoTallerIntegrador\\EntidexEnterprise\\venv\\Lib\\site-packages\\langsmith\\client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: (question goes here) \n",
      "Context: (context goes here) \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "# N.B. for non-US LangSmith endpoints, you may need to specify\n",
    "# api_url=\"https://api.smith.langchain.com\" in hub.pull.\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"(context goes here)\", \"question\": \"(question goes here)\"}\n",
    ").to_messages()\n",
    "\n",
    "assert len(example_messages) == 1\n",
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c13ba767-1e62-4293-bac4-f76a54425b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from typing_extensions import List, TypedDict\n",
    "from langgraph.graph import START, StateGraph\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7bbcd25f-0361-40f3-86db-77c963405583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26471417-fa3f-43b7-a899-5535e35a9a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: [Document(id='9a9c7215-328a-4a9e-bf31-c3e8b4f18918', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'), Document(id='23f72733-70dc-4146-81e1-c8e8c077addf', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'), Document(id='576c2d3c-643f-4576-b664-3d4bb4ff8791', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\n\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.'), Document(id='cc46e13b-3d9a-4434-9efc-c554a294c178', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\n\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.')]\n",
      "\n",
      "\n",
      "Answer: Task decomposition is a method of breaking down a complicated task into smaller, simpler steps. This can be achieved through prompting LLMs, using task-specific instructions, or with human input. It is a standard technique to enhance model performance on complex tasks.\n"
     ]
    }
   ],
   "source": [
    "result = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
    "\n",
    "print(f'Context: {result[\"context\"]}\\n\\n')\n",
    "print(f'Answer: {result[\"answer\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fca83210-0308-409c-a93c-71ebd169f88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'retrieve': {'context': [Document(id='9a9c7215-328a-4a9e-bf31-c3e8b4f18918', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'), Document(id='23f72733-70dc-4146-81e1-c8e8c077addf', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'), Document(id='576c2d3c-643f-4576-b664-3d4bb4ff8791', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\n\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.'), Document(id='cc46e13b-3d9a-4434-9efc-c554a294c178', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\n\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.')]}}\n",
      "\n",
      "----------------\n",
      "\n",
      "{'generate': {'answer': 'Task decomposition involves breaking down a complex task into smaller, simpler steps. This can be achieved through prompting language models, using task-specific instructions, or with human input. Chain of Thought (CoT) and Tree of Thoughts are standard prompting techniques that utilize task decomposition to enhance model performance.'}}\n",
      "\n",
      "----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for step in graph.stream(\n",
    "    {\"question\": \"What is Task Decomposition?\"}, stream_mode=\"updates\"\n",
    "):\n",
    "    print(f\"{step}\\n\\n----------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fab4158-daba-4afe-b384-5068489a433c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task| decomposition is a| method of breaking down complex tasks into smaller, simpler steps. It can be achieved through| prompting language models, using task-specific instructions, or with human input. This approach| is utilized to enhance model performance on intricate tasks by transforming them into manageable sub-tasks.\n",
      "|"
     ]
    }
   ],
   "source": [
    "for message, metadata in graph.stream(\n",
    "    {\"question\": \"What is Task Decomposition?\"}, stream_mode=\"messages\"\n",
    "):\n",
    "    print(message.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3612872d-c13e-446e-9f82-fead82bf9058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea293749-8a4c-436c-9b8d-8b607e650baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       " 'section': 'beginning'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_documents = len(all_splits)\n",
    "third = total_documents // 3\n",
    "\n",
    "for i, document in enumerate(all_splits):\n",
    "    if i < third:\n",
    "        document.metadata[\"section\"] = \"beginning\"\n",
    "    elif i < 2 * third:\n",
    "        document.metadata[\"section\"] = \"middle\"\n",
    "    else:\n",
    "        document.metadata[\"section\"] = \"end\"\n",
    "\n",
    "\n",
    "all_splits[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e823153e-b79d-4aa0-90d7-2be6e8a10a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "_ = vector_store.add_documents(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "453278ea-558c-4304-98a7-b6353da4bbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "\n",
    "class Search(TypedDict):\n",
    "    \"\"\"Search query.\"\"\"\n",
    "\n",
    "    query: Annotated[str, ..., \"Search query to run.\"]\n",
    "    section: Annotated[\n",
    "        Literal[\"beginning\", \"middle\", \"end\"],\n",
    "        ...,\n",
    "        \"Section to query.\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f53a72fb-ce5a-49d0-9854-027364aad100",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    question: str\n",
    "    query: Search\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "def analyze_query(state: State):\n",
    "    structured_llm = llm.with_structured_output(Search)\n",
    "    query = structured_llm.invoke(state[\"question\"])\n",
    "    return {\"query\": query}\n",
    "\n",
    "\n",
    "def retrieve(state: State):\n",
    "    query = state[\"query\"]\n",
    "    retrieved_docs = vector_store.similarity_search(\n",
    "        query[\"query\"],\n",
    "        filter=lambda doc: doc.metadata.get(\"section\") == query[\"section\"],\n",
    "    )\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([analyze_query, retrieve, generate])\n",
    "graph_builder.add_edge(START, \"analyze_query\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e6ac7d4-e0c2-4938-b975-d20e7d881aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c9947ed-2431-4671-a025-16d43a0aef70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'analyze_query': {'query': {'query': 'Task Decomposition', 'section': 'end'}}}\n",
      "\n",
      "----------------\n",
      "\n",
      "{'retrieve': {'context': [Document(id='1132c058-4f54-47c9-aca6-c5c2ca372d84', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'section': 'end'}, page_content='Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.'), Document(id='05c9083d-f356-48fd-821d-ff3de1a68853', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'section': 'end'}, page_content='Resources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.'), Document(id='0ef71cde-bea6-46b3-ad08-ed6a6e7a6b27', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'section': 'end'}, page_content='Or\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\\n“ arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\\n[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023.\\n[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).'), Document(id='7fad4363-ff15-44b4-8a11-5dcc47c10c86', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'section': 'end'}, page_content='[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\\n[12] Parisi et al. “TALM: Tool Augmented Language Models”\\n[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\\n[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer')]}}\n",
      "\n",
      "----------------\n",
      "\n",
      "{'generate': {'answer': 'The post states that planning over a lengthy history and effectively exploring the solution space remain challenging for LLMs. It also mentions that LLMs struggle to adjust plans when faced with unexpected errors. This makes them less robust compared to humans who learn from trial and error in task decomposition.'}}\n",
      "\n",
      "----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for step in graph.stream(\n",
    "    {\"question\": \"What does the end of the post say about Task Decomposition?\"},\n",
    "    stream_mode=\"updates\",\n",
    "):\n",
    "    print(f\"{step}\\n\\n----------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43ff7ee-a6d5-4a64-a9bc-a5d2c9cbcb00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
